---
title: "Modeling the Expert - Machine Learning"
author: "Cathy Kiriakos"
date: "October 10, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r}
#Intro to logistic regression: critical decisions are often made by people with expert knowledge.  Ie.. Healthcare Quality Assessment:  good care educates patients and controls costs, need to assess quality for proper medical interventions, no single set of guidelines for defining quality of healthcare, professionals are experts of care assessment.   Experts are limite by memory and time.  HQA: expert physicians can evaluate quality by examining a patients records, this process is time cosuming and inefficient.   Can we develop analytical tools that replicate expert assessment in large scale?  Learn from human justgment, develop a model interpret results, adjust the model.  Make precdcitions/evaluations.

#Claims Data: Medical/Pharmacy:  electronically available, standardized, not 100% accurate, under reporting is common, claims for hospital visits can be vague.  
#Claims Sample:  large health insurance database, radomly selected 131 diabetes patients, ages from 35-55, costs $10k-20k, 9/1/03-8/31/05

#Claims Sample/Expert Review:  Expert physician reviewed the cliams and wrote descriptive notes:  ongoing narcotic use, only Avandia - not a good first choice drug, had regular visits including mamogram and immunizations, was given home testing supplies.  Rated quality on a 2-point scale (poor/good)  Care was poor - poorly treated diabetes, no eye care but high quality.

#Extract variables:  Dependent: Quality of care, Independent: Diabetes treatment, patient demographics, healthcare utilization, providers, claims, prescriptions.

#Dependent variable is modeled as a binary variable - 1 if low 0 if high - this is a "categorical variable" with a small number of possible outcomes.  

#Linear Regression would predict a continuous outcome.  How can we extend the idea of linear regressuib ti sutyatuibs where the outcome is categorical? *Only want to predict 1 or 0 * Could round outcome to 0 or 1 bud can do better with logistic regression. 

#Logistic Regression will predict the probablity of poor care P(y=1) * where y is poor care
#Then P(y=0)=1-P(y=1)
#Independent variables x1, x2... xn
#Uses logistic response function: p(y=1) = 1/(1+e-(linear regression equation)) ie beta zero + beta1x1+ beta2x2+...betanxn

#positive coefficient values are predictive of class 1
#negative coefficient values are predictive of class 0 - decreases linear regression

# The coefficients or beta values are selected to predict a high probability for the poor care cases, and low probability of the good care cases

#The Logistic function is similar to thinking about odds...   Odds = P(y=1)/P(y=0)
#Odds >1 if y=1 is more likely
#Odds <1 if y=0 is more likely

# The Logit:  turns out that Odds = e^(Beta0+beta1x1+...betanxn)
#A positive beta coefficient increases the odcds P(y=1), where a negative coefficent decreases the odds 
# This is called the logit and looks like linear regression - the bigger the logit, the bigger P(y=1)
```

```{r}
#Model for Healthcare Quality
quality <- read.csv("C:/Users/kiriakosc/Documents/R/quality.csv", header=TRUE)
```

```{r view structure}
#Poor care is the independent variable 
str(quality)
```


```{r}
# Make a table to look at poor care and quality - 98 of 131 had good care, while 33 recieved poor care
table(quality$PoorCare)
```

```{r}
#Consider using a simple baseline method: to predict the most frequent outcome - baseline is 75% (shown beolw) = /total sample
98/131

```

```{r}
#Install 
install.packages("caTools")
```

```{r}
library(caTools)
```

```{r}
#Use package to randomly split data into a testing set and a training set - using function split - set seed initialized random number generator
set.seed(88)
```

```{r}
#use split - so that test set is representative of training set 25% to test set, 75% to training set - this ensures that 75% of both the test and the training set have good care to mimic the baseline
split<- sample.split(quality$PoorCare, SplitRatio = 0.75)
split #to view: T/F True = Obseervation in Traning set False = Obseervation in Test Set
```

```{r}
#Training set: setting up data sets accordingly
qualityTrain <- subset(quality, split == TRUE)
qualityTest <- subset(quality, split == FALSE)
```

```{r}
# Check for total data 
nrow(qualityTrain)
nrow(qualityTest)
```

```{r}
#build logistic regression model - family =  binomial tells r to build a logistic regression model - glm = generalized linear model *** family = binomial tells r to build a logistic regression model
#                 Dependent var - independent vars  -   data set to use
QualityLog <- glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family = binomial) #doesnt tie to tutorial? 
summary(QualityLog)
```

```{r}
#The output above is similar to a linear regression model, you will want to focus on the coefficients table - providing the estimate values for the coeffifcients or betas
# Both positive for office visits and narcotics - higher values are indicitive of poor care 
# they are both significant, because of the *
# AIC like r squared is a measure of the quality of the model, looking at # of variables used compared to the number of observations. It can only be compared between models using the same data set - the perferred model is the one with a minimum AIC
```

```{r}
#Tells the predict function to give a probability - since we're predicting probabilties -- all #'s should be between 0-1
predictTrain <- predict(QualityLog, type = "response")
```

```{r}
#STAT SUMM
summary(predictTrain)
```

```{r}
#Compute average prediction for each of true the outcomes
tapply(predictTrain, qualityTrain$PoorCare, mean)
```

```{r}
#for all of the true poor care cases....  P(0.44) for all of the true good care cases P(0.21) - doesnt line up with MITX???
```

```{r}
#Thresholding:  the outcome of logistic regression model is a probability...  Often a binary prediction, did the patient recieve good or poor care?  
#We can do this using a threshold value t
# if P(PoorCare = 1)>=t, predict poor quality
# if P(PooCare = 1)<t, predict good quality 
#What value should we pick for t? Often selected based on which errors are "better"
#If it is LARGE - predict pooer care rarely whne P(y=1) os ;arge
  # More errors where we say good care, but it is actually poor care
  # Detects all patients who might be recieving poor care
#With no preference between the errors, select t=0.5
  #predicts more likely outcome

#Selecting a threshold value: Compare actual outcomes to predicted outcomes using a confusion or classification matrics; rows = actual outcomes, columns = predicted 
#           Predicted = 0         Predicted = 1
#Actual=0   True Negatives(TN)    False Positives(FP)
#Actual=1   False Negatives(FN)   True Positives (TP)

#Called:   Sensitivity = TP/(TP+FN) - measures the % of actual poor care - aka true positive range 
#           Specificity = TN/(TN+FP) - meausres the % of actual good care - aka true negative range
#A model with a higher threshold will have a lower sensitivity and a higher specificity, a model with a lower threshold will have the inverse

```

```{r}
#Compute confusion matrices in R with different threshold values: 
#True Outcome - rows       ,column names
table(qualityTrain$PoorCare, predictTrain > 0.5) # returns True if prediction is >0.5 - predicting poor care; false will return good care
```

```{r}
#Rows indicate 0 or 1  actual outcomes & the columns indicate predicted outcomes T/F - above 72 cases predicted good care, and recieved good care, and for 7 cases we predicted poor care an actually got poor care; there are 2 mistakes when we say good care and actually recieved poor care; and 17 mistakes where we predicted good care and it was actually poor care
```

```{r}
#Compute sensiitivity for true positive range, and specificity for the true negative range: 
#Sensitivity would be 7/33 - or the total predicted 1's - Sensitivity = 0.28
7/25
```

```{r}
#Compute specificity for true negative range = 72/74 - Specificity = 0.97
72/74
```

```{r}
#Next try increasing the thresholds
table(qualityTrain$PoorCare, predictTrain >0.7)
```

```{r}
#Now Compute Sensitivity & Specificity 
6/25 #Sensitivity = 0.24 (decreased with increased threshold)
73/74 #Specificity = 0.99 (increased with increased threshold)
```

```{r}
#Now we will decrease the threshold: 
table(qualityTrain$PoorCare, predictTrain >0.2)
```

```{r}
18/25 #Sensitivity
48/74 #Specificity
```

```{r}
# With a lower threshold our specificity went down, and our sensitivity went up.  How do you decide which threshold to use? 
```

```{r}
# Reciever Operator Characteristic Curve (ROC): can help determine which threshold value ot assess.  Sensitivity or true positive is on the y-axis of the model; where false positive is on the x-axis of the model.  It will always start at point (0,0) - which corresponds to a threshold value of 1 - if you have a threshold of 1 you will not catch any PoorCare cases or will have a sensitivity of 0; but will correctly label all of the GoodCare cases.  The ROC curve will allways end at the point (1,1) which corresponds to a threshold value of 0 - where you will catch all of the poor care cases, but  you will label all of the GoodCare cases as poor as well, meaning you will have a false positive value of 1.    
# True positive rate (sensitivity) on y-axis
#   Proportion of poor care caught
#False positive rate (1-specificity) on x-axis
#   Proportion of good care labeled as poor care

# The ROC curve captures all thresholds simutaniously
# High Threshold: 
#   High specificity 
#   Low sensitivity
#Low Threshold
#   Low specificity 
#   High Sensitivity

#Choose the best thrshold for the best trade off 
#   Cost of faily to detect positives
#   Costs of raising false alarms

# You can label the threshold values in r by color coding the curve, and by having a legend at the right; you can also add spedic threshold labels to the curve
```

```{r}
#Generate ROC Curve
install.packages("ROCR")
```

```{r}
library(ROCR)
```

```{r}
#Call predict train for ROC Curve 
ROCRpred <- prediction(predictTrain, qualityTrain$PoorCare)
                                #x-axis(TruePositive Rate), y-axis(FalsePositive Rate)
ROCRperf <- performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf)
```

```{r}
#Add colors by adding arguments
plot(ROCRperf, colorize = TRUE)

```

```{r}
#Add threshold lables                           Between 0 & 1 with increments of 0.1
plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
```

```{r}
#Interpreting the model:  multicolinearity could be a problem, do the coefficients make sense? check correlations - if they are excessively high then there might be multicoliniarity in the model, do the signs makes sence do the beta coefficents makes sense - if not there might be a sign of multicoliniarity.
#Significance: how do we interpret the model fit.  
# Look at AUC(area under the curve) - showing the absolute value of the prediction 0.775 percent, perfect = 1.0 so this is a B - where a 0.5 score is like a guess.  Gives an abolute measeur of quality and is less effective by various benchmarks.
#Good AUC? perfect prediction =1 - minimum of 0.5 = pure guessing.  
#Other outcome measures - using a confusion matrix

#           Predicted Class= 0    Predicted Ckass = 1
#Actual Class=0   True Negatives(TN)    False Positives(FP)
#Actual Class=1   False Negatives(FN)   True Positives (TP)

#N = Number of observations
#Overall Accuracy = (TN+TP)/N
#Overall Error Rate = (FP+FN)/N
#Sensitivity = TP/(TP+FN)
#Specificity = TN/(TN+FP)
#False Negative Error Rate = FN/(TP+FN)
#Fales Positive Error Rate = FP/(TN+FP)

#The actual class of 0 = good quality of care, 1 = poor quality care

#Making predictions: just like in linear regression, we want to make predictions on a test set to compute out-of-sample metrics

predictTest <- predict(QualityLog, type = "response", newdata = qualityTest) #this makes predictions for probabilities, if we use a threshold value of 0.3 we get the following confusion matrix
table(qualityTest$PoorCare, predictTest > 0.3) #32 cases: 24 are actually good care, and 8 are poor care, the overall accuracy is (22+7)/32; the false positive rate is 2/24 or 0.08; while the true positive rate is 7/8 or 0.88.  We predict good care all of the time = 24/25 times - but it doesnt captures teh dynamices of the model
```

```{r}
#The analytics Edge: An expert trained model can accurately identify diabetics recieving poor care  - with an out of sample accuracy rate of 78%, identifies most patients recieving poor care.  In practice the probabilites returned by the logistic regression model can be used to prioritize patients for intervention.  Electronic medical records could be used in the future.  

#
```

